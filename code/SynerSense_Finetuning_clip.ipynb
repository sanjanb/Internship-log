{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNyF645CPV5xHJtQt+Gzpl9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanjanb/Internship-log/blob/main/SynerSense_Finetuning_clip.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tring modular approach**"
      ],
      "metadata": {
        "id": "3xNVlDkTO7WM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading and Slicing the Vision Encoder"
      ],
      "metadata": {
        "id": "bS0XpvfTOX_A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fq7kefvxYqiB"
      },
      "outputs": [],
      "source": [
        "from transformers import CLIPModel, CLIPProcessor\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "# Loading pretrained CLIP model\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "# Slicing only the visual encoder\n",
        "vision_encoder = model.vision_model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Passing an Image and Getting its Embeddings"
      ],
      "metadata": {
        "id": "tg-KAXtSOaLs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing a sample image\n",
        "image = Image.open(\"image.png\")\n",
        "\n",
        "# Processing image\n",
        "inputs = processor(images=image, return_tensors=\"pt\")\n",
        "\n",
        "# Extracting visual features\n",
        "with torch.no_grad():\n",
        "    vision_outputs = vision_encoder(**inputs)\n",
        "    pooled_output = vision_outputs.pooler_output  # (1, 768)"
      ],
      "metadata": {
        "id": "x9QclT8gZTlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Connecting to Dense (MLP) Network"
      ],
      "metadata": {
        "id": "gLYOhvHSOoH0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class VLMtoMLP(nn.Module):\n",
        "    def __init__(self, input_dim=768, hidden_dim=256, output_dim=2):\n",
        "        super(VLMtoMLP, self).__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.mlp(x)\n",
        "\n",
        "# Instantiateing and connecting\n",
        "mlp_head = VLMtoMLP()\n",
        "output = mlp_head(pooled_output)"
      ],
      "metadata": {
        "id": "7I4lc81mZYaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding Loss and Train"
      ],
      "metadata": {
        "id": "Fp948kRXOtcK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(mlp_head.parameters(), lr=1e-4)\n",
        "\n",
        "# Dummy labels for example\n",
        "labels = torch.tensor([1])\n",
        "\n",
        "loss = criterion(output, labels)\n",
        "loss.backward()\n",
        "optimizer.step()\n"
      ],
      "metadata": {
        "id": "wcKzHEYGZbZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Testing for a single image**"
      ],
      "metadata": {
        "id": "8kujUh8KOE9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import CLIPModel, CLIPProcessor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from PIL import Image\n",
        "\n",
        "# Device setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load CLIP model\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "# Slice visual encoder\n",
        "vision_encoder = model.vision_model\n",
        "for param in vision_encoder.parameters():\n",
        "    param.requires_grad = False  # Freeze if not training\n",
        "\n",
        "# Load image\n",
        "image = Image.open(\"image.png\")\n",
        "inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# Forward pass\n",
        "with torch.no_grad():\n",
        "    vision_outputs = vision_encoder(**inputs)\n",
        "    pooled_output = vision_outputs.pooler_output  # shape: [1, 768]\n",
        "\n",
        "# Custom MLP head\n",
        "class VLMtoMLP(nn.Module):\n",
        "    def __init__(self, input_dim=768, hidden_dim=256, output_dim=2):\n",
        "        super(VLMtoMLP, self).__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.mlp(x)\n",
        "\n",
        "# Train step\n",
        "mlp_head = VLMtoMLP().to(device)\n",
        "output = mlp_head(pooled_output)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(mlp_head.parameters(), lr=1e-4)\n",
        "\n",
        "labels = torch.tensor([1], dtype=torch.long).to(device)\n",
        "\n",
        "loss = criterion(output, labels)\n",
        "loss.backward()\n",
        "optimizer.step()\n",
        "\n",
        "print(\"Loss:\", loss.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hk8igMfXMmI2",
        "outputId": "ee2ba9f7-0de7-4f7a-a503-3fc57683d3b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.8742982149124146\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Code for Dataset**\n",
        "Since dataset is being prepared, the model haven't trained on it"
      ],
      "metadata": {
        "id": "xjdL1Lo6Pr6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Load pretrained CLIP\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "# Freeze the vision encoder\n",
        "for param in model.vision_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "vision_encoder = model.vision_model\n",
        "\n",
        "# Custom MLP head\n",
        "class VLMtoMLP(nn.Module):\n",
        "    def __init__(self, input_dim=768, hidden_dim=256, output_dim=2):\n",
        "        super(VLMtoMLP, self).__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.mlp(x)\n",
        "\n",
        "mlp_head = VLMtoMLP().to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(mlp_head.parameters(), lr=1e-4)\n",
        "\n",
        "# Transform (CLIP expects 224x224 RGB images normalized)\n",
        "transform = Compose([\n",
        "    Resize((224, 224)),\n",
        "    ToTensor(),\n",
        "    Normalize((0.4815, 0.4578, 0.4082), (0.2686, 0.2613, 0.2758))  # CLIP normalization\n",
        "])\n",
        "\n",
        "# Load dataset\n",
        "dataset = datasets.ImageFolder(root=\"dataset\", transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(5):\n",
        "    total_loss = 0\n",
        "    for images, labels in dataloader:\n",
        "        # CLIP expects pixel_values in a dict\n",
        "        inputs = processor(images=[transforms.ToPILImage()(img) for img in images],\n",
        "                           return_tensors=\"pt\", padding=True)\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Get visual embeddings\n",
        "        with torch.no_grad():\n",
        "            vision_outputs = vision_encoder(**inputs)\n",
        "            pooled_output = vision_outputs.pooler_output  # (B, 768)\n",
        "\n",
        "        # Forward pass\n",
        "        logits = mlp_head(pooled_output)\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: Loss = {total_loss:.4f}\")"
      ],
      "metadata": {
        "id": "RyQ7Oi3LQLQa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
