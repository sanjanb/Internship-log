---
layout: default
title: "Multimodal Pretraining for Vision-and-Language Tasks"
parent: "Learnings"
nav_order: 1
---

# Multimodal Pretraining for Vision-and-Language Tasks

**Paper**: [arXiv:2103.00020](https://arxiv.org/abs/2103.00020)  
**Summary**: This paper explores how joint vision-language representation learning improves transfer learning.

## Key Ideas

- Uses contrastive learning on paired image-text data.
- Improves tasks like VQA, image captioning, etc.
- Incorporates visual encoders (ViT) and text encoders (BERT-like).

## Reflections

- Helped me relate my internship work (image + instruction-based generation) with foundational ideas in representation fusion.

> _“This paper helped me understand why multimodal embeddings are crucial for semantic alignment.”_
